version: "3.4"

x-spark-default-env: &spark-default-env
  SPARK_NO_DAEMONIZE: true
  SPARK_SCALA_VERSION: 2.13

x-spark-default: &spark-default
  env_file: .env
  volumes:
    - ./conf/app.properties:/opt/spark/conf/app.properties
    - ./conf/log4j2.properties:/opt/spark/conf/log4j2.properties
    - ./conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf

x-spark-worker-default-env: &spark-worker-default-env
  <<: *spark-default-env
  SPARK_MASTER: spark://spark-master:7077
  SPARK_WORKER_DIR: /opt/spark/work-dir

x-spark-submit-default-env: &spark-submit-default-env
  <<: *spark-default-env
  SPARK_MASTER_HOST: spark-master
  SPARK_MASTER_PORT: 7077
  DEPLOY_MODE: cluster

volumes:
  mosquitto_data:
  zookeeper_data:
  zookeeper_log:
  kafka_data:
  opensearch-node1:
  opensearch-node2:

services:
  mosquitto:
    image: eclipse-mosquitto:2.0.15
    volumes:
      - ./conf/mosquitto.conf:/mosquitto/config/mosquitto.conf:ro
      - ./conf/mosquitto_passwd:/mosquitto/config/password_file:ro
      - mosquitto_data:/mosquitto/data
    ports:
      - 1883:1883
    deploy:
      mode: replicated
      replicas: 1
      restart_policy:
        condition: on-failure
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
        reservations:
          cpus: "0.25"
          memory: 128M

  mqtt-source:
    image: mataelang/kafka-mqtt-source:1.1
    container_name: mqtt-source
    environment:
      MQTT_HOST: mosquitto
      MQTT_PORT: 1883
      MQTT_USERNAME: mataelang
      MQTT_PASSWORD: mataelang
      MQTT_TOPIC: mataelang/sensor/v3/+
      KAFKA_BOOSTRAP_SERVERS: kafka:9092
      KAFKA_PRODUCE_TOPIC: sensor_events
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 64M
        reservations:
          cpus: "0.25"
          memory: 32M

  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_log:/var/lib/zookeeper/log
    deploy:
      mode: replicated
      replicas: 1
      restart_policy:
        condition: on-failure
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
        reservations:
          cpus: "0.25"
          memory: 256M

  kafka:
    image: confluentinc/cp-kafka:7.3.0
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_LOG_RETENTION_BYTES: 536870912 # 512MB
      KAFKA_LOG_RETENTION_MS: 86400000 # 1 day
      KAFKA_LOG_SEGMENT_BYTES: 536870912 # 512MB
    volumes:
      - kafka_data:/var/lib/kafka/data
    deploy:
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: "0.5"
          memory: 2G
        reservations:
          cpus: "0.25"
          memory: 1G

  control-center:
    image: provectuslabs/kafka-ui
    container_name: control-center
    depends_on:
      - zookeeper
      - kafka
      - mqtt-source
    ports:
      - "9021:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: MataElangKafkaCluster
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    deploy:
      resources:
        limits:
          cpus: "0.50"
          memory: 768M
        reservations:
          cpus: "0.25"
          memory: 384M

  spark-master:
    <<: *spark-default
    image: mataelang/spark:3.3.1-scala2.13
    depends_on:
      - kafka
    ports:
      - "8080:8080"
    environment:
      <<: *spark-default-env
      SPARK_MASTER_HOST: spark-master
      SPARK_MASTER_PORT: 7077
      SPARK_MASTER_WEBUI_PORT: 8080
      SPARK_DAEMON_MEMORY: 1g
    command: >
      bash -c "/opt/spark/sbin/start-master.sh"
    deploy:
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: "1"
          memory: 1G
        reservations:
          cpus: "1"
          memory: 1G

  spark-worker:
    <<: *spark-default
    image: mataelang/spark:3.3.1-scala2.13
    depends_on:
      - spark-master
    environment:
      <<: *spark-worker-default-env
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 4G
    command: >
      bash -c "/opt/spark/sbin/start-worker.sh $$SPARK_MASTER"
    deploy:
      mode: replicated
      replicas: 2

  spark-historyserver:
    <<: *spark-default
    image: mataelang/spark:3.3.1-scala2.13
    depends_on:
      - spark-master
    ports:
      - target: 18080
        published: 18080
        protocol: tcp
        mode: host
    environment:
      <<: *spark-default-env
      SPARK_DAEMON_MEMORY: 1G
    command: >
      bash -c "/opt/spark/sbin/start-history-server.sh"
    deploy:
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: "1"
          memory: 1G
        reservations:
          cpus: "1"
          memory: 1G

  spark-submit-enrich:
    <<: *spark-default
    image: mataelang/spark:3.3.1-scala2.13
    restart: "no"
    depends_on:
      - spark-master
      - spark-worker
    working_dir: /opt/spark
    environment:
      <<: *spark-submit-default-env
      TOTAL_EXECUTOR_CORES: 1
      SPARK_DRIVER_MEMORY: 1g
      SPARK_EXECUTOR_CORES: 1
      SPARK_EXECUTOR_MEMORY: 1g
      SPARK_APP_UI: 4040
      SPARK_APP_CLASSNAME: org.mataelang.kaspacore.jobs.SensorEnrichDataStreamJob
      SPARK_APP_NAME: SensorEnrichDataStreamJob
    command: >
      bash -c "/opt/spark/bin/spark-submit \\
          --class $$SPARK_APP_CLASSNAME \\
          --name $$SPARK_APP_NAME \\
          --total-executor-cores $$TOTAL_EXECUTOR_CORES \\
          --conf spark.ui.port=$$SPARK_APP_UI \\
          --conf spark.submit.deployMode=$$DEPLOY_MODE \\
          --conf spark.driver.memory=$$SPARK_DRIVER_MEMORY \\
          --conf spark.executor.cores=$$SPARK_EXECUTOR_CORES \\
          --conf spark.executor.memory=$$SPARK_EXECUTOR_MEMORY \\
          --conf spark.eventLog.dir=$$SPARK_EVENTLOG_DIR \\
          --files conf/app.properties \\
          $$SPARK_APP_JAR_PATH
          "

  spark-submit-aggr:
    <<: *spark-default
    image: mataelang/spark:3.3.1-scala2.13
    restart: "no"
    depends_on:
      - spark-master
      - spark-worker
    working_dir: /opt/spark
    environment:
      <<: *spark-submit-default-env
      TOTAL_EXECUTOR_CORES: 1
      SPARK_DRIVER_MEMORY: 2g
      SPARK_EXECUTOR_CORES: 1
      SPARK_EXECUTOR_MEMORY: 2g
      SPARK_SHUFFLE_PARTITION: 1
      SPARK_APP_UI: 4041
      SPARK_APP_CLASSNAME: org.mataelang.kaspacore.jobs.SensorAggregationStreamJob
      SPARK_APP_NAME: SensorAggregationStreamJob
    command: >
      bash -c "/opt/spark/bin/spark-submit \\
          --class $$SPARK_APP_CLASSNAME \\
          --name $$SPARK_APP_NAME \\
          --total-executor-cores $$TOTAL_EXECUTOR_CORES \\
          --conf spark.ui.port=$$SPARK_APP_UI \\
          --conf spark.submit.deployMode=$$DEPLOY_MODE \\
          --conf spark.driver.memory=$$SPARK_DRIVER_MEMORY \\
          --conf spark.executor.cores=$$SPARK_EXECUTOR_CORES \\
          --conf spark.executor.memory=$$SPARK_EXECUTOR_MEMORY \\
          --conf spark.eventLog.dir=$$SPARK_EVENTLOG_DIR \\
          --conf spark.sql.shuffle.partitions=$$SPARK_SHUFFLE_PARTITION \\
          --conf spark.sql.codegen.aggregate.map.twolevel.enabled=false \\
          --conf spark.sql.streaming.metricsEnabled=true \\
          --files conf/app.properties \\
          $$SPARK_APP_JAR_PATH
          "

  opensearch-node1:
    image: opensearchproject/opensearch:2.4.0
    environment:
      - cluster.name=opensearch-cluster
      - node.name=opensearch-node1
      - discovery.seed_hosts=opensearch-node1,opensearch-node2
      - cluster.initial_cluster_manager_nodes=opensearch-node1,opensearch-node2
      - bootstrap.memory_lock=true
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - opensearch-data1:/usr/share/opensearch/data
    ports:
      - 9200:9200

  opensearch-node2:
    image: opensearchproject/opensearch:2.4.0
    environment:
      - cluster.name=opensearch-cluster
      - node.name=opensearch-node2
      - discovery.seed_hosts=opensearch-node1,opensearch-node2
      - cluster.initial_cluster_manager_nodes=opensearch-node1,opensearch-node2
      - bootstrap.memory_lock=true
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - opensearch-data2:/usr/share/opensearch/data

  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:2.4.0
    ports:
      - 5601:5601
    environment:
      OPENSEARCH_HOSTS: '["https://opensearch-node1:9200","https://opensearch-node2:9200"]'

  opensearch-logstash:
    image: opensearchproject/logstash-oss-with-opensearch-output-plugin:8.4.0
    command: "-f /usr/share/logstash/config/pipeline.conf"
    ports:
      - "5044:5044"
      - "5000:5000/tcp"
      - "5000:5000/udp"
      - "9600:9600"
    environment:
      LS_JAVA_OPTS: -Xmx256m -Xms256m
      LOGSTASH_INTERNAL_PASSWORD: ${LOGSTASH_INTERNAL_PASSWORD:-}
    volumes:
      - ./conf/pipeline.conf:/usr/share/logstash/config/pipeline.conf
